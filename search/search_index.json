{"config": {"lang": ["en"], "separator": "[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Supercharge your Terminal and Markdown workflow with AI", "text": "<p>Use NavamAI to supercharge your productivity and workflow with personal, fast, and quality AI. Turn your Terminal into a configurable, interactive, and unobtrusive personal AI app. Power of 15 LLMs and 7 providers at your finger tips. Pair with workflows in Markdown, VS Code, Obsidian, and GitHub. Get productive fast with three simple commands.</p>"}, {"location": "#automate-markdown-workflows", "title": "Automate Markdown Workflows", "text": "<p>NavamAI works with markdown content (text files with simple formatting commands). So you can use it with many popular tools like VS Code and Obsidian to quickly and seamlessly design a custom workflow that enhances your craft.</p> <p></p>"}, {"location": "#create-situational-apps", "title": "Create Situational Apps", "text": "<p>You can create, install, and run fully functional web apps with NavamAI. Situational apps on your command!</p> <p>Just run <code>ask</code> command to select the app creation prompt template.</p> <p></p> <p>NavamAI runs the app on your laptop within seconds. You don't need to do any setup, touch a single line of code, or invest in any service.</p> <p></p>"}, {"location": "#quick-start", "title": "Quick Start", "text": "<p>Go to a folder where you want to initialize NavamAI. This could be your Obsidian vault or a VC Code project folder or even an empty folder.</p> <pre><code>pip install -U navamai # upgrade or install latest NavamAI\nnavamai init # copies config file, quick start samples\nnavamai id # identifies active provider and model\nask \"How old is the oldest pyramid?\" # start prompting the model\n</code></pre>"}, {"location": "#command-is-all-you-need", "title": "Command Is All You Need", "text": "<p>So, the LLM science fans will get the pun in our tagline. It is a play on the famous paper that introduced the world to Transformer model architecture - Attention is all you need. With NavamAI a simple command via your favorite terminal or shell is all you need to bend an large or small language model to your wishes. </p> <p>NavamAI provides a rich UI right there within your command prompt. No browser tabs to open, no apps to install, no context switching... just pure, simple, fast workflow. Try it with a simple command like <code>ask \"create a table of planets\"</code> and see your Terminal come to life just like a chat UI with fast streaming responses, markdown formatted tables, and even code blocks with color highlights if your prompt requires code in response! </p> <p>NavamAI has released 14 commands to help customize your personal AI workflow.</p> <p>*Note that <code>navamai</code>, <code>ask</code> and <code>refer</code> are three top level commands available to you when you install NavamAI.</p> Command Example and Description *ask <code>ask \"your prompt\"</code>Prompt the LLM for a fast, crisp (default up to 300 words), single turn response<code>ask</code>Browses the configured prompts folder, lists prompt templates for user to run. audit <code>navamai audit</code>Analyze your own usage of NavamAI over time with an insightful command line dashboard and markdown report. config <code>navamai config ask save true</code>Edit <code>navamai.yml</code> file config from command line gather <code>navamai gather \"webpage url\"</code>Cleanly scrape an article from a webpage, without the menus, sidebar, footer. Includes images. Saves as markdown formatted similar to the HTML source. Refer the scraped markdown content using <code>refer gather</code> command.Use vision on scraped images using <code>navamai vision</code> command. *refer <code>refer text-to-extract</code>Browse a set of raw text files and run custom prompts to extract new structured content.<code>refer inline-prompts-to-run</code>Run prompts embedded inline within text to expand, change, and convert existing documents.<code>refer intents</code>Expand a set of intents and prompts within an intents template<code>refer your-command</code>You can configure your own extensions to refer command by simply copying and changing any of the existing <code>refer-existing</code> model configs. run <code>navamai run</code>Browse <code>Code</code> folder for markdown files with code blocks, create, setup, and run the app in one single command!The <code>Code</code> folder markdown files with code blocks are created using <code>ask</code> command running <code>Create Vite App.md</code> prompt template or similar. id <code>navamai id</code>Identifies the current provider and model for <code>ask</code> command<code>navamai id section-name</code>Identifies the provider and model defined in specific section init <code>navamai init</code>Initialize navamai in any folder. Copies <code>navamai.yml</code> default config and quick start Intents and Embeds folders and files. Checks before overwriting. Use --force option to force overwrite files and folders. intents <code>navamai intents \"Financial Analysis\"</code>Interactively choose from a list of intents within a template to refer into content embeds merge <code>navamai merge \"Source File\"</code>Finds <code>Source File updated</code> with placeholder tags <code>[merge here]</code> or as custom defined in <code>merge</code> config, then merges the two files into <code>Source File merged</code>. Use along with <code>refer inline-prompts-to-run</code> command to reduce number of tokens processed when editing long text for context but updating only a section. split <code>navamai split \"Large File\"</code>Use this command to split a large file into chunks within a specified ratio of target model context. You can configure target model and ratio in <code>split</code> section of the configuration. Original file remains untouched and new files with same name and <code>part #</code> suffix are created. test <code>navamai test ask</code>Tests navamai command using all providers and models defined in <code>navamai.yml</code> config and provides a test summary.<code>navamai test vision</code>Test vision models. trends <code>navamai trends</code>Visualize latency and token length trends based on <code>navamai test</code> runs for <code>ask</code> and <code>vision</code> commands across models and providers. You can trend for a period of days using  <code>--days 7</code> command option. validate <code>navamai validate \"Financial Analysis\"</code>Validates prior generated embeds running another model and reports the percentage difference between validated and original content. vision <code>navamai vision -p path/to/image.png \"Describe this image\"</code>Runs vision models on images from local path (-p), url (-u), or camera (-c) and responds based on prompt. <p>There is no behavioral marketing or growth hacking a business can do within your command prompt. You guide your workflow the way you feel fit. Run the fastest model provider (Groq with Llama 3.1), or the most capable model right now (Sonnet 3.5 or GPT-4o), or the latest small model on your laptop (Mistral Nemo), or the model with the largest context (Gemini 1.5 Flash), you decide. Run with fast wifi or no-wifi (using local models), no constraints. Instantly search, research, Q&amp;A to learn something or generate a set of artifacts to save for later. Switching to any of these workflows is a couple of changes in a config file or a few easy to remember commands on your terminal.</p>"}, {"location": "#quick-pitch", "title": "Quick Pitch", "text": "<p>Please watch our super exciting one minute product pitch video! Made with \u2764\ufe0f using NavamAI and iMovie.</p> <p></p>"}, {"location": "#do-more-with-less", "title": "Do More With Less", "text": "<p>NavamAI is very simple to use out of the box as you learn its handful of powerful commands. As you get comfortable you can customize NavamAI commands simply by changing one configuration file and align NavamAI to suit your workflow. Everything in NavamAI has sensible defaults to get started quickly.</p>"}, {"location": "#make-it-your-own", "title": "Make It Your Own", "text": "<p>When you are ready, everything is configurable and extensible including commands, models, providers, prompts, model parameters, folders, and document types. Another magical thing happens when the interface to your generative AI is a humble command prompt. You will experience a sense of being in control. In control of your workflow, your privacy, your intents, and your artifacts. You are completely in control of your personal AI workflow with NavamAI.</p>"}, {"location": "features/actionable-intents/", "title": "Actionable Intents", "text": "<p>Your intents are tasks you want to execute, goals you want to accomplish, plans you want to realize, decisions you want to make, or questions you want to answer. You control your entire NavamAI experience with your intents.</p> <p></p> <p>You can save your intents as simple outline of tasks in a text file. You can recall them when you need. You can run models on your intents as you feel fit. You can save results based on your intents.</p> <p></p>", "tags": ["Intents", "Content Generation"]}, {"location": "features/actionable-intents/#combining-navamai-commands", "title": "Combining NavamAI Commands", "text": "<p>When combined with other NavamAI commands this workflow can get even more powerful. As an example, start by defining your document template for a set of intents and prompts as a simple markdown. For example <code>Financial Analysis</code> or <code>Product Management</code> are shown here. Next add a few intents as headings like, <code>Macro Factors Impact Stocks</code> or <code>Top Companies by ROCE</code> and so on. Then add simple prompts under these intents to generate content. You can now use NavamAI to expand on the set of intents and prompts in your document template with the command <code>refer intents-to-expand \"Financial Analysis\"</code> and the model will brainstorm more related intents and prompts for you to use.</p> <p></p> <p>Now run <code>navamai intents-to-expand \"Financial Analysis\"</code> and choose among a list of intents to generate as content embeds. The response is saved under <code>Embeds</code> folder automatically and the embed is linked in your document template instantly. Rinse, repeat.</p> <p>This workflow can get really useful very fast. As each template has linked embeds, Obsidian Graph view can be used to visualize the links. You can get creative and link related templates or even enhance generated embeds with more intents. Of course this also means you can use all the great Obsidian plugins to generate websites, PDFs, and more.  Your creativity + Obsidian + NavamAI = Magic!</p> <p></p>", "tags": ["Intents", "Content Generation"]}, {"location": "features/analyze-personal-ai/", "title": "Analyze Personal AI", "text": "<p>Prompts and commands you use over time represent your Personal AI. NavamAI enables you to analyze time trail of commands and prompts.</p> <p> NavamAI saves a trail of commands, prompts, templates, lookup folders, and saved files in <code>trail.yml</code> file. You can visualize this anytime using <code>navamai audit</code> command to gain insights of your NavamAI usage over time.</p> <p></p> <p>You can run this command over time to continue understanding more about your usage patterns.</p> <p></p>", "tags": ["Analytics", "Personal AI"]}, {"location": "features/compare-models/", "title": "Compare Models", "text": "<p>NavamAI comes with configurable support for more than 15 leading models from five providers (Ollama, Anthropic, OpenAI, Groq, Google). The <code>navamai test</code> command can be used to run each of the navamai commands over all the provider and model combinations and respond with a summary of model test and evaluation results. Use this to quickly compare models and providers as well as when you add or remove a new model or provider in the config.</p> <p></p> <p>This command is super useful when comparing model response time (latency), response quality (does it follow the system and prompt instructions), response accuracy, and token length (cost) for the same prompt. You can configure the test prompts within <code>navamai.yml</code> in the <code>test</code> section.</p>", "tags": ["Models", "Benchmark"]}, {"location": "features/compare-models/#run-multiple-models-side-by-side", "title": "Run multiple models side by side", "text": "<p>Want to compare multiple models side by side? All you need to do is open multiple shells or Terminal instances. Now in each of these, one by one, change the model, run same <code>ask \"prompt\"</code> and compare the results side by side. Simple!</p> <p></p> <p></p> <p>As NavamAI commands use the <code>navamai.yml</code> config in the current folder every time they run, you can create more complex parallel running, multi-model and cross-provider workflows by simply copying the config file into multiple folders and running commands there. This way you can be running some long running tasks on a local model in one folder and terminal. While you are doing your day to day workflow in another. And so on.</p>", "tags": ["Models", "Benchmark"]}, {"location": "features/compare-models/#test-and-evaluate-models-and-providers", "title": "Test and Evaluate Models and Providers", "text": "<p>NavamAI comes with configurable support for more than 15 leading models from five providers (Ollama, Anthropic, OpenAI, Groq, Google). The <code>navamai test</code> command can be used to run each of the navamai commands over all the provider and model combinations and respond with a summary of model test and evaluation results. Use this to quickly compare models and providers as well as when you add or remove a new model or provider in the config.</p> <p>This command is super useful when comparing model response time (latency), response quality (does it follow the system and prompt instructions), response accuracy, and token length (cost) for the same prompt. You can configure the test prompts within <code>navamai.yml</code> in the <code>test</code> section.</p> <p></p> <p>Here is an example of running <code>navamai test vision</code> command and resulting test summary. I this default prompt and image we are sharing image of around 150-160 people standing in close proximity in a circle and asking the model to count the number of people. The right number is between 150-160. This can be used to calculate the relative accuracy of each model based on the response. How closely the response follows the system prompt and the user prompts is  indicative of quality of response.</p> <p>You can also notice the response times seem proportional to model size. For Claude, Opus &gt; Sonnet &gt; Haiku. For Gemini, Pro &gt; Flash. For OpenAI, GPT-4o &gt; GPT-4-mini.</p> <p>You can similarly run <code>navamai test ask</code> command to test across all models and providers. In this run you may find groq is among the fastest providers when it comes to response time.</p> <p>Of course, you may need multiple test runs to get better intuition of response times as there are multiple factors which effect latency other than model size or architecture, like network latency, which may change across multiple test runs.</p>", "tags": ["Models", "Benchmark"]}, {"location": "features/create-apps/", "title": "Create Apps", "text": "<p>You can create, install, and run fully functional web apps with NavamAI. Situational apps on your command!</p> <p>Just run <code>ask</code> command to select the app creation prompt template.</p> <p></p> <p>Then NavamAI generates the app code as a markdown blog with code blocks. Edit if you please or simply run <code>navamai run</code> command to run the app. That simple!</p> <p></p> <p>NavamAI runs the app on your laptop within seconds. You don't need to do any setup, touch a single line of code, or invest in any service.</p> <p></p>", "tags": ["Apps", "Code Generation"]}, {"location": "features/gather-insights/", "title": "Gather Insights", "text": "<p>We also launched <code>navamai gather</code> command recently. You can now scrape webpages easily and analyze these with <code>refer gather</code> command or use <code>navamai vision</code> command on downloaded images within the gathered pages.</p> <p></p>", "tags": ["Analytics", "Dataset Creation"]}, {"location": "features/privacy-controls/", "title": "Privacy Controls", "text": "<p>You decide which model and provider you trust, or even choose to run an LLM locally on your laptop. You are in control of how private your data and preferences remain. NavamAI supports state of the art models from Anthropic, OpenAI, Google, and Meta. You can choose a hosted provider or Ollama as a local model provider on your laptop. Switch between models and providers using a simple command like <code>navamai config ask model llama</code> to switch from the current model.</p> <p>You can also load custom model config sets mapped to each command. Configure these in <code>navamai.yml</code> file. Here is an example of constraining how <code>navamai ask</code> and <code>navamai intents</code> commands behave differently using local and hosted model providers.</p> <pre><code>ask:\n  provider: ollama\n  model: mistral\n  max-tokens: 300\n  save: false\n  system: Be crisp in your response. Only respond to the prompt \n    using valid markdown syntax. Do not explain your response.\n  temperature: 0.3\n\nintents:\n  provider: claude\n  model: sonnet\n  max-tokens: 1000\n  save: true\n  folder: Embeds\n  system: Only respond to the prompt using valid markdown syntax.\n    When responding with markdown headings start at level 2. \n    Do not explain your response.\n  temperature: 0.0\n</code></pre>", "tags": ["Privacy", "Personal AI"]}, {"location": "features/prompt-templates/", "title": "Prompt Templates", "text": "<p>NavamAI released new set of features to make your terminal even more powerful. Once you install NavamAI you can simply run <code>ask</code> command and it will browse your configured folder containing predefined prompt templates. Once you select a template, if it contains variables, it will ask you for variable values. Then it runs the prompt with filled variables values and streams a well formatted response using your choice of model, right there within the same terminal.</p> <p></p> <p>In the above example we created a prompt template with detailed instructions for analyzing a stock symbol and reporting insights and BUY/SELL/HOLD recommendation. The template can run on any valid stock symbol provided when the command is run. Response can be saved in a file by turning on save configuration within the <code>navamai.yml</code> file.</p> <p>We have also added the capability to include <code>{{PROMPT_VARIABLES}}</code> within your prompt templates. You can also include <code>{{TEXT_FILE}}</code> special prompt variable. As NavamAI processes your prompt template it asks you for variable values and if it encounters and text file variable it lists files in the configured lookup folder for you to choose from. Then NavamAI injects the variable values or file content within your prompt before running it with the chosen model and provider. Simple!</p> <p>You can also create prompt templates over time by looking up the <code>trail.yml</code> audit trail logs of all the prior NavamAI commands you have run recording the custom prompts you have used in the past. If you find you are using similar prompts over and over with variations, then these are candidates for prompt templates with variables, as an example.</p> <p></p> <p>Similar to <code>ask</code> command available as a top level command, we have also enabled <code>refer</code> command as a top level command to save you a few more keystrokes.</p> <p>Hope you enjoy this update. We would love to hear your feedback. suggestions for ways to improve NavamAI for your use cases are always welcome. Please leave us a comment or DM us on twitter. Thank you.</p>", "tags": ["Prompt Engineering"]}, {"location": "features/seamless-workflow/", "title": "Seamless Workflow", "text": "<p>NavamAI works with markdown content (text files with simple formatting commands). So you can use it with many popular tools like VS Code and Obsidian to quickly and seamlessly design a custom workflow that enhances your craft.</p> <p></p> <p>Obsidian supports the beautiful Mermaid charts in markdown code blocks. NavamAI can generate these charts based on topics and relationships described in a long text given as input.</p> <p>We start by creating a prompt template like so. During <code>navamai init</code> this prompt template is already included in your Prompts folder.</p> <pre><code>Carefully read the reference text to understand the \nworkflows, topics, and relationships between topics described. \nYour job is to create a single or a set of mermaid {{DIAGRAM_TYPE}}, \neach in their own code block. \nThe {{DIAGRAM_TYPE}} should be drawn in portrait mode, vertically. \nThe code block(s) should be tagged as mermaid code. \nUse appropriate background and high contrast text colors \nto make the text readable.\nUse topic matching icons if the mermaid diagram supports it.\nMake sure text does not overlap.\nCreate a newline after each code block and specify an \nappropriate caption for the {{DIAGRAM_TYPE}}. \nDo not explain your response.\n\nReference Text:\n\n{{TEXT_FILE}}\n</code></pre> <p>Now all we need to do is point this prompt template at the right raw text and within seconds NavamAI creates magic within your Obsidian rendered markdown note.</p> <p>The prompt template has a <code>{{DIAGRAM_TYPE}}</code> variable which NavamAI asks you to specify. You can choose from many Mermaid diagram types including flow chart, mind map, and quadrant chart. This is what we use in the above example. Next the prompt template has an <code>{{TEXT_FILE}}</code> variable which NavamAI asks for you to select. That's it.</p>", "tags": ["Workflow Automation", "Content Authoring"]}, {"location": "features/trending-models/", "title": "Trending Models", "text": "<p>Each <code>navamai test</code> command run saves the test summary data in <code>Metrics</code> folder by timestamp and provider-model. Over time you can visualize trends of latency and token count metrics to see if models are performing consistently. Run <code>navamai trends</code> command to view trends for 7 days (default).</p> <p></p> <p>The visualization uses sparklines to show data trends over time. Use this to decide when/if to switch models based on performance trends.</p>", "tags": ["Model Evaluation"]}, {"location": "features/vision-models/", "title": "Vision Models", "text": "<p>NavamAI also supports vision models. You can access your Mac synced phone camera right there from a single terminal command. How cool is that!</p> <p></p>", "tags": ["Vision Models"]}, {"location": "guides/models-environment-setup/", "title": "Models and Environment Setup", "text": "<p>This guide will help you get setup if you are getting started with NavamAI without any prior setup of generative AI models or Python environment.</p>", "tags": ["models", "python"]}, {"location": "guides/models-environment-setup/#models-setup", "title": "Models Setup", "text": "<p>You will need to setup model-provider keys. Edit your environment config <code>~/.zshrc</code> like so.</p> <pre><code>export ANTHROPIC_API_KEY= # https://docs.anthropic.com/en/api/getting-started\nexport OPENAI_API_KEY= # https://openai.com/index/openai-api/\nexport GROQ_API_KEY= # https://console.groq.com/keys\nexport GEMINI_API_KEY= # https://ai.google.dev/gemini-api\nexport PERPLEXITY_KEY= # https://www.perplexity.ai/settings/api\n</code></pre> <p>If you do not want to use any of the model then all you need to do is remove the corresponding entries from <code>navamai.yml</code> in the <code>model-mapping</code> and the <code>provider-model-mapping</code> sections. Also ensure that the other model configs only refer to available models.</p> <p>For local models install Ollama and download the latest models you want to run.</p> <p>Now you are ready to test all models and providers.</p> <pre><code>navamai test ask\nnavamai test vision\n</code></pre>", "tags": ["models", "python"]}, {"location": "guides/models-environment-setup/#python-environment-setup-optional", "title": "Python Environment Setup (optional)", "text": "<p>Skip this section if you have your Python environment already working.</p> <p>First, you should be running the latest Python on your system with Python package manager upgraded to the latest.</p> <pre><code>python --version\n# should return Python 3.12.x or higher as on Sep'24\n</code></pre> <ol> <li>Optionally install latest Python on Mac. </li> <li>Safely install NavamAI. Follow this thread to setup <code>pyenv</code> version manager.</li> </ol> <p>First change to the directory to your VS Code workspace, Obsidian vault, or folder where you want to install NavamAI. Next create the virtual environment.</p> <pre><code>python -m venv env\n</code></pre> <p>Now you can activate your virtual environment like so. You will notice that directory prefixed with <code>(env)</code> to indicate you are now running in the virtual environment.</p> <pre><code>. env/bin/activate\n</code></pre> <p>To leave the virtual environment using <code>deactivate</code> command. Re-enter using same command as earlier.</p> <p>Now you are ready to install NavamAI.</p>", "tags": ["models", "python"]}, {"location": "tags/", "title": "Tags", "text": ""}, {"location": "tags/#analytics", "title": "Analytics", "text": "<ul> <li>Analyze Personal AI</li> <li>Gather Insights</li> </ul>"}, {"location": "tags/#apps", "title": "Apps", "text": "<ul> <li>Create Apps</li> </ul>"}, {"location": "tags/#benchmark", "title": "Benchmark", "text": "<ul> <li>Compare Models</li> </ul>"}, {"location": "tags/#code-generation", "title": "Code Generation", "text": "<ul> <li>Create Apps</li> </ul>"}, {"location": "tags/#content-authoring", "title": "Content Authoring", "text": "<ul> <li>Seamless Workflow</li> </ul>"}, {"location": "tags/#content-generation", "title": "Content Generation", "text": "<ul> <li>Actionable Intents</li> </ul>"}, {"location": "tags/#dataset-creation", "title": "Dataset Creation", "text": "<ul> <li>Gather Insights</li> </ul>"}, {"location": "tags/#intents", "title": "Intents", "text": "<ul> <li>Actionable Intents</li> </ul>"}, {"location": "tags/#model-evaluation", "title": "Model Evaluation", "text": "<ul> <li>Trending Models</li> </ul>"}, {"location": "tags/#models", "title": "Models", "text": "<ul> <li>Compare Models</li> </ul>"}, {"location": "tags/#personal-ai", "title": "Personal AI", "text": "<ul> <li>Analyze Personal AI</li> <li>Privacy Controls</li> </ul>"}, {"location": "tags/#privacy", "title": "Privacy", "text": "<ul> <li>Privacy Controls</li> </ul>"}, {"location": "tags/#prompt-engineering", "title": "Prompt Engineering", "text": "<ul> <li>Prompt Templates</li> </ul>"}, {"location": "tags/#vision-models", "title": "Vision Models", "text": "<ul> <li>Vision Models</li> </ul>"}, {"location": "tags/#workflow-automation", "title": "Workflow Automation", "text": "<ul> <li>Seamless Workflow</li> </ul>"}, {"location": "tags/#models_1", "title": "models", "text": "<ul> <li>Models and Environment Setup</li> </ul>"}, {"location": "tags/#python", "title": "python", "text": "<ul> <li>Models and Environment Setup</li> </ul>"}]}